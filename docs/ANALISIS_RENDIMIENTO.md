# üìä An√°lisis de Rendimiento - PC Price Scraper

**Fecha:** 13 de Noviembre 2025  
**Versi√≥n:** 1.0

---

## üîç An√°lisis de MemoryKings

### ‚è±Ô∏è Tiempos de Scraping (Estimados)

#### Configuraci√≥n Actual (run.py):
- **MAX_LISTADOS:** 20 listados por categor√≠a
- **MAX_PRODUCTS:** 30 productos por listado
- **Categor√≠as:** 4 (procesadores, tarjetas-video, memorias-ram, almacenamiento)

#### C√°lculo de Tiempo:

```
Tiempo por producto: 
  - P√°gina de listado: 3s (wait_time)
  - P√°gina de producto: 5s (wait_time) + 0.5s (sleep)
  - Total por producto: ~8.5s

Por categor√≠a:
  - Productos esperados: 20 listados √ó 30 productos = 600 productos
  - Tiempo: 600 √ó 8.5s = 5,100s = 85 minutos
  - Con sleeps adicionales: ~90-95 minutos

Total (4 categor√≠as):
  - Tiempo estimado: 4 √ó 90min = 360 minutos = 6 HORAS
```

### üêå Problemas Identificados

#### 1. **Selenium con Wait Times Largos**
```python
# scraper.py l√≠nea 90
soup = super().fetch_page(listado_url, wait_time=3)  # 3s por listado

# scraper.py l√≠nea 109
soup = super().fetch_page(product_url, wait_time=5)  # 5s por producto
```

**Impacto:** Cada producto requiere 5 segundos de espera aunque el contenido cargue en 1s.

#### 2. **Procesamiento Secuencial**
```python
# scraper.py l√≠neas 318-328
for url in product_urls:
    product = self.scrape_product_page(url)
    if product:
        all_products.append(product)
    time.sleep(0.5)  # Rate limiting adicional
```

**Impacto:** Un producto a la vez, no hay paralelizaci√≥n.

#### 3. **Muchos Listados Configurados**
- **Procesadores:** 13 listados (Ryzen 3000, 4000, 5000, 7000, 8000, 9000, Intel 10¬™, 12¬™, 14¬™, etc.)
- **Tarjetas de video:** 9 listados (RTX 5000 series, AMD RX, Intel Arc)
- **Memorias RAM:** 7 listados (DDR3, DDR4, DDR5 variants)
- **Almacenamiento:** 8 listados (SSD Gen3/4/5, HDD)

**Total:** 37 listados √ó 30 productos = 1,110 requests potenciales

#### 4. **Extracci√≥n Innecesaria de Datos**
```python
# scraper.py l√≠neas 148-165
# Busca imagen en 3 m√©todos diferentes aunque og:image funcione
# Busca marca en logo aunque pueda extraerse del nombre
```

---

## ‚ö° Optimizaciones Propuestas

### 1. **Reducir Wait Times (Ganancia: ~60%)**

```python
# Optimizaci√≥n: Esperar solo lo necesario
soup = super().fetch_page(listado_url, wait_time=1)  # 3s ‚Üí 1s
soup = super().fetch_page(product_url, wait_time=2)  # 5s ‚Üí 2s
```

**Impacto:** 8.5s/producto ‚Üí 3.5s/producto = **~4 horas menos**

### 2. **Scraping Paralelo con ThreadPoolExecutor (Ganancia: ~70%)**

```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def scrape_category_parallel(self, category_key, max_workers=5):
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = []
        for url in product_urls:
            future = executor.submit(self.scrape_product_page, url)
            futures.append(future)
        
        for future in as_completed(futures):
            product = future.result()
            if product:
                all_products.append(product)
```

**Impacto:** 5 productos simult√°neos = **1.2 horas total** (en vez de 6 horas)

### 3. **Scraping desde Listados (Ganancia: ~80%)**

En lugar de visitar cada p√°gina de producto, extraer datos directamente del listado:

```python
def scrape_category_quick(self, url):
    """Scraping r√°pido desde listado (como SercoPlus)"""
    soup = self.fetch_page(url)
    products = []
    
    for product_card in soup.find_all('div', class_='product-item'):
        # Extraer precio directamente
        price_elem = product_card.find('span', class_='price')
        # Extraer nombre
        name_elem = product_card.find('h2', class_='product-title')
        # Crear producto sin visitar p√°gina individual
        products.append(self.create_product_dict(...))
    
    return products
```

**Impacto:** Solo visita listados (37 p√°ginas) = **~3-5 minutos total**

### 4. **Cach√© de Resultados**

```python
import hashlib
from pathlib import Path

def fetch_page_cached(self, url, cache_hours=24):
    cache_file = Path(f"cache/{hashlib.md5(url.encode()).hexdigest()}.html")
    
    if cache_file.exists():
        age = time.time() - cache_file.stat().st_mtime
        if age < cache_hours * 3600:
            return BeautifulSoup(cache_file.read_text(), 'html.parser')
    
    soup = self.fetch_page(url)
    cache_file.parent.mkdir(exist_ok=True)
    cache_file.write_text(str(soup))
    return soup
```

**Impacto:** Runs subsecuentes = **~0 segundos** (usa cache)

### 5. **Reducir Listados (Ganancia: ~50%)**

Enfocarse en listados con m√°s productos:

```python
# Priorizar listados principales
priority_listados = {
    'procesadores': [
        'Ryzen 5000 Series',  # M√°s vendidos
        'Ryzen 7000 Series',  # √öltimos
        'Intel Core 12¬™ Gen',
        'Intel Core 14¬™ Gen'
    ]
}
```

**Impacto:** 37 listados ‚Üí 15 listados = **3 horas menos**

---

## üè™ Comparaci√≥n con SercoPlus

### SercoPlus (M√°s R√°pido):
```python
# sercoplus_scraper.py l√≠nea 173
soup = self.fetch_page(page_url, wait_time=5)  # Solo para listados

# Extrae datos del listado directamente (l√≠neas 183-221)
for container in product_containers:
    # No visita p√°ginas individuales
    product = extract_from_card(container)
```

**Ventajas:**
- ‚úÖ Solo visita p√°ginas de listados (con paginaci√≥n)
- ‚úÖ Extrae precio, nombre, SKU del HTML del listado
- ‚úÖ Maneja paginaci√≥n autom√°ticamente
- ‚úÖ ~2-3 minutos por categor√≠a

**Tiempo Total SercoPlus:** ~10-15 minutos para todas las categor√≠as

### MemoryKings (M√°s Lento):
- ‚ùå Visita cada p√°gina de producto individualmente
- ‚ùå Wait time de 5s por producto
- ‚ùå Procesamiento secuencial
- ‚ùå ~90 minutos por categor√≠a

**Tiempo Total MemoryKings:** ~6 horas para todas las categor√≠as

---

## üéØ Recomendaciones Inmediatas

### Opci√≥n 1: Quick Wins (Implementar YA) ‚ö°
1. **Reducir wait_times:** 5s ‚Üí 2s (Ganancia: 3 horas)
2. **Reducir listados:** 37 ‚Üí 20 prioritarios (Ganancia: 2 horas)
3. **Reducir productos por listado:** 30 ‚Üí 15 (Ganancia: 1.5 horas)

**Resultado:** 6 horas ‚Üí 1.5 horas (75% m√°s r√°pido)

**Cambios en run.py:**
```python
MAX_LISTADOS = 10  # 20 ‚Üí 10
MAX_PRODUCTS = 15  # 30 ‚Üí 15
```

**Cambios en scraper.py:**
```python
soup = super().fetch_page(listado_url, wait_time=1)  # 3s ‚Üí 1s
soup = super().fetch_page(product_url, wait_time=2)  # 5s ‚Üí 2s
```

### Opci√≥n 2: Refactor Completo (Mejor a largo plazo) üî®
1. **Implementar scraping paralelo** con ThreadPoolExecutor
2. **Scraping desde listados** como SercoPlus
3. **Cach√© de p√°ginas** visitadas
4. **Detecci√≥n inteligente de carga** (no wait times fijos)

**Resultado:** 6 horas ‚Üí 5-10 minutos (98% m√°s r√°pido)

---

## üåê B√∫squeda de P√°ginas Similares a SercoPlus

### Caracter√≠sticas deseadas:
‚úÖ Listados con precios visibles sin JavaScript  
‚úÖ Paginaci√≥n simple  
‚úÖ Estructura HTML consistente  
‚úÖ Datos completos en tarjetas de producto  
‚úÖ Sin CAPTCHA o protecciones anti-bot  

### Tiendas Peruanas Recomendadas:

#### 1. **PC Factory** (https://www.pcfactory.cl - Per√∫)
- ‚úÖ Listados claros con precios
- ‚úÖ Paginaci√≥n est√°ndar
- ‚úÖ Similar a SercoPlus
- ‚ö†Ô∏è Requiere verificar stock en Per√∫

#### 2. **Oechsle** (https://www.oechsle.pe - Tecnolog√≠a)
- ‚úÖ Tienda retail con secci√≥n PC
- ‚úÖ HTML simple
- ‚ö†Ô∏è Cat√°logo limitado en componentes

#### 3. **Linio Per√∫** (https://www.linio.com.pe)
- ‚úÖ Marketplace con m√∫ltiples vendedores
- ‚úÖ Precios en soles
- ‚ö†Ô∏è Calidad variable de datos

#### 4. **Phantom** (https://www.phantomcomputers.com)
- ‚úÖ Tienda especializada en gaming
- ‚úÖ Cat√°logo completo
- ‚ö†Ô∏è Requiere an√°lisis de estructura HTML

#### 5. **Xtreme PC** (https://xtremepc.com.pe)
- ‚úÖ Especializado en componentes
- ‚úÖ Precios competitivos
- ‚ö†Ô∏è Verificar estructura de listados

#### 6. **PC Gamer** (https://pcgamer.com.pe)
- ‚úÖ Enfoque en gaming
- ‚úÖ Stock local
- ‚ö†Ô∏è Analizar HTML primero

### Criterios de Selecci√≥n:

**Prioridad Alta:**
1. Estructura HTML similar a SercoPlus
2. Precios visibles en listados (no requiere JS)
3. Sin CAPTCHA
4. Stock actualizado

**Prioridad Media:**
5. Cat√°logo de 500+ productos
6. Paginaci√≥n funcional
7. Im√°genes de calidad

**Bonus:**
8. API p√∫blica
9. Sitemap XML disponible
10. Datos estructurados (JSON-LD)

---

## üìù Pr√≥ximos Pasos

### Inmediato (Hoy):
1. ‚úÖ Reducir MAX_LISTADOS y MAX_PRODUCTS en run.py
2. ‚úÖ Reducir wait_times en scraper.py
3. ‚è≥ Ejecutar scraping optimizado de MemoryKings
4. ‚è≥ Medir tiempo real vs estimado

### Corto Plazo (Esta Semana):
1. ‚è≥ Investigar 2-3 tiendas de la lista recomendada
2. ‚è≥ Crear scrapers similares a SercoPlus
3. ‚è≥ Implementar scraping paralelo b√°sico
4. ‚è≥ Agregar cach√© de p√°ginas

### Mediano Plazo (Pr√≥ximas 2 Semanas):
1. ‚è≥ Refactor completo de MemoryKings (estilo SercoPlus)
2. ‚è≥ Dashboard con comparaci√≥n de tiendas
3. ‚è≥ Sistema de notificaciones de ofertas
4. ‚è≥ Hist√≥rico de precios

---

## üìä M√©tricas de √âxito

| M√©trica | Actual | Meta |
|---------|--------|------|
| Tiempo total | ~6 horas | ~15 minutos |
| Productos por hora | ~185 | ~4,000 |
| Wait time promedio | 5s | 1s |
| √âxito de extracci√≥n | ~85% | ~95% |
| Uso de CPU | Bajo (secuencial) | Medio (paralelo) |
| Cacheabilidad | 0% | 80% |

---

**Generado por:** GitHub Copilot  
**√öltima actualizaci√≥n:** 2025-11-13
